{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Glossary:\n",
    "\n",
    "1. **`pd.merge()`**  \n",
    "    **Use**: Merge two dataframes based on common columns or indices.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    merged_data = pd.merge(df1, df2, on='common_column')\n",
    "    ```\n",
    "\n",
    "2. **`head()`**  \n",
    "    **Use**: Return the first `n` rows of a dataframe. By default, `n=5`.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    first_rows = df.head()\n",
    "    ```\n",
    "\n",
    "3. **`groupby()`**  \n",
    "    **Use**: Group the dataframe using a particular column, then apply a function (like sum, mean, or count).  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    grouped = df.groupby('column_name').sum()\n",
    "    ```\n",
    "\n",
    "4. **`sort_values()`**  \n",
    "    **Use**: Sort by the values along either axis.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    sorted_df = df.sort_values(by='column_name', ascending=False)\n",
    "    ```\n",
    "\n",
    "5. **`reset_index()`**  \n",
    "    **Use**: Reset the index of the dataframe. Often used after groupby operations.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    reset_df = grouped.reset_index()\n",
    "    ```\n",
    "\n",
    "6. **`.loc[]` with Boolean Indexing**  \n",
    "    **Use**: Modify a subset of a DataFrame in place. Helps avoid \"SettingWithCopyWarning\".  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df.loc[df['column_name'] < value, 'other_column'] = new_value\n",
    "    ```\n",
    "\n",
    "7. **`iloc[]`**  \n",
    "    **Use**: Purely integer-location based indexing for selection by position.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    first_row = df.iloc[0]\n",
    "    ```\n",
    "\n",
    "8. **`nunique()`**  \n",
    "    **Use**: Return the number of unique elements in the object.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    unique_count = df['column_name'].nunique()\n",
    "    ```\n",
    "\n",
    "9. **Boolean Indexing**  \n",
    "    **Use**: Filter the dataframe using a condition or a set of conditions.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    filtered_df = df[df['column_name'] < value]\n",
    "    ```\n",
    "\n",
    "10. **`transform()`**  \n",
    "    **Use**: Perform an operation on each group in a dataframe and return data in its original shape. Commonly used with `groupby` for operations like normalization within groups.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df['normalized_column'] = df.groupby('group_column')['data_column'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    ```\n",
    "\n",
    "11. **`query()`**  \n",
    "    **Use**: Filter dataframe based on a query expression. Often more readable with complex conditions.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    result = df.query(\"(col1 > 10 & col2 < 5) | col3 == 'value'\")\n",
    "    ```\n",
    "\n",
    "12. **`isin()`**  \n",
    "    **Use**: Filter based on multiple possible values in a column.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    values = ['value1', 'value2']\n",
    "    filtered_df = df[df['col'].isin(values)]\n",
    "    ```\n",
    "\n",
    "13. **`pd.read_csv()` and `to_csv()`**  \n",
    "    **Use**: Read from and write to CSV files.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df = pd.read_csv('filename.csv')\n",
    "    df.to_csv('output_filename.csv', index=False)\n",
    "    ```\n",
    "\n",
    "14. **`pd.read_excel()` and `to_excel()`**  \n",
    "    **Use**: Read from and write to Excel files.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df = pd.read_excel('filename.xlsx', sheet_name='Sheet1')\n",
    "    df.to_excel('output_filename.xlsx', sheet_name='Sheet1', index=False)\n",
    "    ```\n",
    "\n",
    "15. **`pd.DataFrame()`**  \n",
    "    **Use**: Create a DataFrame from lists, dictionaries, or other data sources.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df = pd.DataFrame({'col1': [1, 2, 3], 'col2': ['A', 'B', 'C']})\n",
    "    ```\n",
    "\n",
    "16. **`describe()`**  \n",
    "    **Use**: Generate descriptive statistics of a DataFrame.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    summary = df.describe()\n",
    "    ```\n",
    "\n",
    "17. **`drop()`**  \n",
    "    **Use**: Drop specified columns or rows from a DataFrame.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df_dropped = df.drop(['column_name'], axis=1)\n",
    "    ```\n",
    "\n",
    "18. **`fillna()`**  \n",
    "    **Use**: Fill NA/NaN values using the specified method.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df_filled = df.fillna(value=0)\n",
    "    ```\n",
    "\n",
    "19. **`dropna()`**  \n",
    "    **Use**: Remove missing values.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df_cleaned = df.dropna()\n",
    "    ```\n",
    "\n",
    "20. **`set_index()`**  \n",
    "    **Use**: Set the DataFrame's index using a particular column.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    df_indexed = df.set_index('column_name')\n",
    "    ```\n",
    "\n",
    "21. **`value_counts()`**  \n",
    "    **Use**: Compute a histogram of a categorical column.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    counts = df['column_name'].value_counts()\n",
    "    ```\n",
    "\n",
    "22. **`unique()`**  \n",
    "    **Use**: Find unique values of a column.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    unique_values = df['column_name'].unique()\n",
    "    ```\n",
    "\n",
    "23. **`pivot_table()`**  \n",
    "    **Use**: Create a spreadsheet-style pivot table.  \n",
    "    **Example**:  \n",
    "    ```python\n",
    "    pivot = df.pivot_table(index='col1', columns='col2', values='col3', aggfunc='sum')\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `pd.merge()`\n",
    "\n",
    "**Use**: Merge two dataframes based on common columns or indices.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "merged_data = pd.merge(df1, df2, on='common_column')\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "Imagine you work at a company and have two datasets:\n",
    "\n",
    "1. One dataset (`employees`) contains employee details like `employee_id`, `name`, and `department_id`.\n",
    "2. Another dataset (`departments`) contains department details like `department_id` and `department_name`.\n",
    "\n",
    "You want to create a unified dataset that lists each employee along with their corresponding department name.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "employees = pd.DataFrame({\n",
    "    'employee_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'department_id': [101, 102, 101]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'department_id': [101, 102],\n",
    "    'department_name': ['HR', 'Finance']\n",
    "})\n",
    "\n",
    "merged_data = pd.merge(employees, departments, on='department_id')\n",
    "print(merged_data)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "   employee_id     name  department_id department_name\n",
    "0            1    Alice            101              HR\n",
    "1            3  Charlie            101              HR\n",
    "2            2      Bob            102         Finance\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `head()`\n",
    "\n",
    "**Use**: Return the first `n` rows of a dataframe. By default, `n=5`.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "first_rows = df.head()\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "Imagine you're a data scientist working with a large dataset of customer purchase records. Before diving into deeper analysis, you want a quick glimpse at the first few rows of the dataset to get a feel for the columns and the types of data they contain.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "purchase_data = pd.DataFrame({\n",
    "    'purchase_id': range(1, 11),\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Phone', 'Tablet', 'Headphones', 'Charger', 'USB Cable', 'Webcam'],\n",
    "    'purchase_price': [1000, 20, 50, 150, 800, 250, 100, 15, 5, 70]\n",
    "})\n",
    "\n",
    "print(purchase_data.head())\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "   purchase_id product_name  purchase_price\n",
    "0            1       Laptop            1000\n",
    "1            2        Mouse              20\n",
    "2            3     Keyboard              50\n",
    "3            4      Monitor             150\n",
    "4            5        Phone             800\n",
    "```\n",
    "\n",
    "Here, using the `head()` method gives you a quick peek at the first 5 rows of your purchase dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `groupby()`\n",
    "\n",
    "**Use**: Group the dataframe using a particular column, then apply a function (like sum, mean, or count).\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "grouped = df.groupby('column_name').sum()\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You work for a retail company and have a dataset of sales data. The dataset contains records of sales for various products across different regions. You wish to calculate the total sales for each region to determine which region is generating the most revenue.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "sales_data = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'A', 'C', 'B', 'C', 'A'],\n",
    "    'region': ['North', 'East', 'South', 'North', 'East', 'West', 'West'],\n",
    "    'sales': [100, 150, 200, 50, 300, 250, 400]\n",
    "})\n",
    "\n",
    "total_sales_by_region = sales_data.groupby('region')['sales'].sum()\n",
    "print(total_sales_by_region)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "region\n",
    "East     450\n",
    "North    150\n",
    "South    200\n",
    "West     650\n",
    "Name: sales, dtype: int64\n",
    "```\n",
    "\n",
    "Here, using the `groupby()` method allows you to see the total sales by region. The result shows that the 'West' region had the highest sales, followed by 'East', 'South', and 'North'.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sort_values()`\n",
    "\n",
    "**Use**: Sort by the values along either axis.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "sorted_df = df.sort_values(by='column_name', ascending=False)\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a school teacher and have a dataset containing scores of students from a recent test. You want to sort the dataset based on the scores in descending order to see which students scored the highest.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "scores_data = pd.DataFrame({\n",
    "    'student_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'score': [85, 92, 78, 90, 88]\n",
    "})\n",
    "\n",
    "sorted_scores = scores_data.sort_values(by='score', ascending=False)\n",
    "print(sorted_scores)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "  student_name  score\n",
    "1          Bob     92\n",
    "3        David     90\n",
    "4          Eve     88\n",
    "0        Alice     85\n",
    "2      Charlie     78\n",
    "```\n",
    "\n",
    "By using the `sort_values()` method, you can clearly see the rank of students based on their test scores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `reset_index()`\n",
    "\n",
    "**Use**: Reset the index of a DataFrame, and use the default one instead. Optional parameters allow fine control of the output.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "df_reset = df.reset_index(drop=True)\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're managing an online bookstore. After filtering out books published before the year 2000, you notice that the DataFrame index is out of order. You'd like to reset the index for better visualization.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "books_data = pd.DataFrame({\n",
    "    'title': ['Book A', 'Book B', 'Book C', 'Book D', 'Book E'],\n",
    "    'publication_year': [1995, 2003, 1987, 2020, 2010]\n",
    "})\n",
    "\n",
    "# Filtering books published after 2000\n",
    "new_books = books_data[books_data['publication_year'] > 2000]\n",
    "\n",
    "# Resetting the index\n",
    "new_books_reset = new_books.reset_index(drop=True)\n",
    "print(new_books_reset)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "    title  publication_year\n",
    "0  Book B              2003\n",
    "1  Book D              2020\n",
    "2  Book E              2010\n",
    "```\n",
    "\n",
    "By using `reset_index()`, you've tidied up the DataFrame's index, making it sequential and more readable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `loc[]`\n",
    "\n",
    "**Use**: Access a group of rows and columns by label(s) or a boolean array.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "subset_df = df.loc[df['column_name'] > threshold_value]\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a sports analyst with data on players' performances. You want to filter out players who scored more than 10 goals in a season to focus on the top performers.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "player_data = pd.DataFrame({\n",
    "    'player_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'goals_scored': [5, 12, 8, 15, 11]\n",
    "})\n",
    "\n",
    "top_scorers = player_data.loc[player_data['goals_scored'] > 10]\n",
    "print(top_scorers)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "  player_name  goals_scored\n",
    "1         Bob            12\n",
    "3       David            15\n",
    "4         Eve            11\n",
    "```\n",
    "\n",
    "By using the `loc[]` method, you can easily filter and focus on the players who scored more than 10 goals.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `iloc[]`\n",
    "\n",
    "**Use**: Purely integer-location based indexing for selection by position.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "subset_df = df.iloc[0:5]\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're working on a project where you've just received a large dataset. Before performing any operations, you want to take a look at the first 5 rows to understand the structure and types of data.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "data = pd.DataFrame({\n",
    "    'A': range(1, 11),\n",
    "    'B': range(11, 21),\n",
    "    'C': range(21, 31)\n",
    "})\n",
    "\n",
    "first_five_rows = data.iloc[0:5]\n",
    "print(first_five_rows)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "   A   B   C\n",
    "0  1  11  21\n",
    "1  2  12  22\n",
    "2  3  13  23\n",
    "3  4  14  24\n",
    "4  5  15  25\n",
    "```\n",
    "\n",
    "Using the `iloc[]` method, you've easily selected the first five rows of the dataframe to get a quick overview.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nunique()`\n",
    "\n",
    "**Use**: Count distinct observations over requested axis.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "unique_values = df['column_name'].nunique()\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "Imagine you work for an e-commerce platform and have transaction data for different products. You're curious about how many unique products have been sold.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "transaction_data = pd.DataFrame({\n",
    "    'transaction_id': range(1, 11),\n",
    "    'product_id': [101, 102, 103, 102, 101, 104, 105, 103, 105, 106],\n",
    "    'amount': [20, 30, 25, 30, 20, 50, 45, 25, 45, 60]\n",
    "})\n",
    "\n",
    "unique_products_sold = transaction_data['product_id'].nunique()\n",
    "print(f\"Number of unique products sold: {unique_products_sold}\")\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "Number of unique products sold: 6\n",
    "```\n",
    "\n",
    "Using the `nunique()` method, you can quickly determine that 6 unique products were sold on the platform.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing\n",
    "\n",
    "**Use**: Allows for filtering data based on specific conditions, producing a subset of rows that adhere to the criteria defined by the boolean expression.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "filtered_df = df[df['column_name'] > threshold_value]\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a financial analyst reviewing the quarterly performance of different company branches. You have a dataset with earnings from each branch, and you want to filter out only those branches that have exceeded a revenue of $500,000 during the quarter.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "company_data = pd.DataFrame({\n",
    "    'branch_name': ['North', 'South', 'East', 'West'],\n",
    "    'quarterly_revenue': [550000, 480000, 520000, 490000]\n",
    "})\n",
    "\n",
    "# Filtering branches with quarterly revenue greater than $500,000\n",
    "high_earning_branches = company_data[company_data['quarterly_revenue'] > 500000]\n",
    "print(high_earning_branches)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "  branch_name  quarterly_revenue\n",
    "0       North             550000\n",
    "2        East             520000\n",
    "```\n",
    "\n",
    "Through boolean indexing, you've easily isolated the branches that have surpassed the revenue benchmark, enabling focused analysis on top-performing segments.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `transform()`\n",
    "\n",
    "**Use**: Transform a group of values with a function. Useful when you want to broadcast a grouped operation result back to the original dataframe.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "df['new_column'] = df.groupby('column_name')['another_column'].transform('sum')\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You manage a store and have sales data for different products over various months. You'd like to calculate the total monthly sales and assign that value to each product record within that month (to maybe calculate each product's share of the total sales for that month).\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "sales_data = pd.DataFrame({\n",
    "    'month': ['Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Mar', 'Mar'],\n",
    "    'product': ['A', 'B', 'A', 'B', 'C', 'A', 'C'],\n",
    "    'sales': [100, 150, 200, 250, 300, 350, 400]\n",
    "})\n",
    "\n",
    "sales_data['monthly_total_sales'] = sales_data.groupby('month')['sales'].transform('sum')\n",
    "print(sales_data)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "  month product  sales  monthly_total_sales\n",
    "0   Jan       A    100                  250\n",
    "1   Jan       B    150                  250\n",
    "2   Feb       A    200                  750\n",
    "3   Feb       B    250                  750\n",
    "4   Feb       C    300                  750\n",
    "5   Mar       A    350                  750\n",
    "6   Mar       C    400                  750\n",
    "```\n",
    "\n",
    "With the `transform()` method, you can see the monthly total sales next to each product's sales for that month, making further calculations (like market share) straightforward.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `query()`\n",
    "\n",
    "**Use**: Query the dataframe based on a condition expressed as a string. Useful for readability when dealing with complex conditions.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "filtered_df = df.query(\"column_name > threshold_value\")\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a meteorologist analyzing weather data. You want to filter days when the temperature was above 30°C and humidity was below 50%.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "weather_data = pd.DataFrame({\n",
    "    'date': ['2021-07-01', '2021-07-02', '2021-07-03', '2021-07-04', '2021-07-05'],\n",
    "    'temperature': [28, 32, 35, 29, 33],\n",
    "    'humidity': [55, 45, 40, 58, 42]\n",
    "})\n",
    "\n",
    "hot_dry_days = weather_data.query(\"temperature > 30 and humidity < 50\")\n",
    "print(hot_dry_days)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "         date  temperature  humidity\n",
    "1  2021-07-02           32        45\n",
    "2  2021-07-03           35        40\n",
    "4  2021-07-05           33        42\n",
    "```\n",
    "\n",
    "By using the `query()` method, you've filtered out the days that met the conditions of having temperatures greater than 30°C and humidity below 50%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `isin()`\n",
    "\n",
    "**Use**: Filter data frames. It returns a boolean Series showing whether each element in the Series is contained in the passed list of values. This is especially useful when you want to filter data based on multiple potential values in a column.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "filtered_df = df[df['column_name'].isin(list_of_values)]\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a librarian managing a catalog of books. You have a list of authors who have won a prestigious literature award, and you wish to filter out the books in your catalog written by these award-winning authors.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "book_catalog = pd.DataFrame({\n",
    "    'book_title': ['A Tale of Two Cities', 'Moby Dick', 'Beloved', 'The Great Gatsby', 'War and Peace'],\n",
    "    'author': ['Charles Dickens', 'Herman Melville', 'Toni Morrison', 'F. Scott Fitzgerald', 'Leo Tolstoy']\n",
    "})\n",
    "\n",
    "award_winning_authors = ['Toni Morrison', 'Leo Tolstoy']\n",
    "\n",
    "# Filtering books by award-winning authors\n",
    "award_winning_books = book_catalog[book_catalog['author'].isin(award_winning_authors)]\n",
    "print(award_winning_books)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "          book_title          author\n",
    "2             Beloved  Toni Morrison\n",
    "4        War and Peace     Leo Tolstoy\n",
    "```\n",
    "\n",
    "By using `isin()`, you've efficiently filtered the books written by award-winning authors, enabling a curated display or promotion around these acclaimed works.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pd.read_csv()`\n",
    "\n",
    "**Use**: It's a function to read a comma-separated values (csv) file into a pandas DataFrame. It provides a variety of options to read data correctly, handle missing values, parse dates, and more.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "df = pd.read_csv('path_to_file.csv')\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a meteorologist and have received a dataset containing daily weather data for the past year in the CSV format. This dataset has details like temperature, humidity, and precipitation. You need to load this data into a pandas DataFrame for analysis.\n",
    "\n",
    "**Example Code**:  \n",
    "(For demonstration purposes, I'll illustrate the concept using a small sample of what the data might look like, rather than an actual file read.)\n",
    "\n",
    "```python\n",
    "# Sample CSV Content (stored in a file named \"weather_data.csv\"):\n",
    "\"\"\"\n",
    "date,temperature,humidity,precipitation\n",
    "2022-01-01,23,45,0\n",
    "2022-01-02,25,43,1\n",
    "2022-01-03,24,44,0\n",
    "\"\"\"\n",
    "\n",
    "# Loading the CSV file into a DataFrame\n",
    "weather_df = pd.read_csv('weather_data.csv')\n",
    "print(weather_df)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "         date  temperature  humidity  precipitation\n",
    "0  2022-01-01           23        45              0\n",
    "1  2022-01-02           25        43              1\n",
    "2  2022-01-03           24        44              0\n",
    "```\n",
    "\n",
    "Using `pd.read_csv()`, you've effortlessly loaded the weather dataset into a pandas DataFrame, making it ready for various analytical operations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pd.read_excel()`\n",
    "\n",
    "**Use**: This function allows you to read an Excel file into a pandas DataFrame. It's perfect for when data is provided in the common `.xlsx` or `.xls` formats. With this function, you can specify sheets, handle missing data, and more.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "df = pd.read_excel('path_to_file.xlsx', sheet_name='Sheet1')\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a school administrator, and you've been provided with an Excel file containing student scores for different subjects. Each sheet in the Excel file represents scores for a particular grade level. You're interested in analyzing the scores for 10th grade students.\n",
    "\n",
    "**Example Code**:  \n",
    "(For the purpose of this demonstration, I'll describe the concept using a hypothetical Excel file named \"student_scores.xlsx\".)\n",
    "\n",
    "```python\n",
    "# Imaginary Excel Content in the sheet named \"10th_Grade\":\n",
    "\"\"\"\n",
    "student_name,math,science,english\n",
    "Alice,85,89,90\n",
    "Bob,80,78,88\n",
    "Charlie,78,85,92\n",
    "\"\"\"\n",
    "\n",
    "# Loading the specific sheet from the Excel file into a DataFrame\n",
    "tenth_grade_scores = pd.read_excel('student_scores.xlsx', sheet_name='10th_Grade')\n",
    "print(tenth_grade_scores)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "  student_name  math  science  english\n",
    "0        Alice    85       89       90\n",
    "1          Bob    80       78       88\n",
    "2      Charlie    78       85       92\n",
    "```\n",
    "\n",
    "With `pd.read_excel()`, you've conveniently imported the 10th grade students' scores into a DataFrame, making the data ready for further analysis or visualization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pd.DataFrame()`\n",
    "\n",
    "**Use**: The `pd.DataFrame()` constructor is used to create a DataFrame object, which is a 2-dimensional labeled data structure with columns that can be of different types (similar to a spreadsheet or SQL table). It's generally understood as a mutable, size-flexible tabular data structure.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "df = pd.DataFrame(data, columns=['column_name1', 'column_name2'])\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "Imagine you're a sports statistician, and you're manually collecting scores from today's basketball games. You want to quickly input the game results into a DataFrame for later analysis and sharing.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "# Data for basketball games\n",
    "teams = ['Lakers', 'Warriors', 'Bulls', 'Knicks']\n",
    "points_scored = [110, 112, 108, 104]\n",
    "\n",
    "# Creating a DataFrame\n",
    "basketball_scores = pd.DataFrame({\n",
    "    'Team': teams,\n",
    "    'Points': points_scored\n",
    "})\n",
    "\n",
    "print(basketball_scores)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "       Team  Points\n",
    "0    Lakers     110\n",
    "1  Warriors     112\n",
    "2     Bulls     108\n",
    "3    Knicks     104\n",
    "```\n",
    "\n",
    "By using `pd.DataFrame()`, you've rapidly organized the basketball game results into a neat table-like structure, setting you up for any subsequent statistical analysis or report generation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `describe()`\n",
    "\n",
    "**Use**: The `describe()` method is applied to pandas DataFrame or Series to generate a summary of the central tendency, dispersion, and shape of the distribution of a dataset. For numeric data, it will provide count, mean, standard deviation, min, 25th, 50th (median), 75th percentiles, and max. For object data (like strings or timestamps), `describe()` outputs the count, unique, top, and freq.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "summary = df['column_name'].describe()\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "Imagine you're a car sales manager, and you have a dataset of the prices at which cars were sold last month. You want to understand the overall distribution of these prices — such as the average price, the range of prices, and the most common price.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "# Sample data of car sale prices\n",
    "car_prices = pd.Series([30000, 32000, 28000, 34000, 27000, 33000, 31000, 29000])\n",
    "\n",
    "# Getting a summary of the car sale prices\n",
    "price_summary = car_prices.describe()\n",
    "print(price_summary)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "count        8.0\n",
    "mean     30500.0\n",
    "std       2408.3\n",
    "min      27000.0\n",
    "25%      28500.0\n",
    "50%      30500.0\n",
    "75%      32250.0\n",
    "max      34000.0\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "With the `describe()` method, you now have a concise summary of the car sale prices, giving you valuable insights into how they're distributed and what pricing trends might be present.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `drop()`\n",
    "\n",
    "**Use**: The `drop()` method is used to drop specified labels from rows or columns of a DataFrame. Essentially, it allows you to remove unwanted columns or rows based on label names or index.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "df_dropped = df.drop(columns=['column_name'])\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a data scientist working on a project involving a dataset of patient records. The dataset contains several columns, including 'Name', 'Age', 'Gender', and 'SSN' (Social Security Number). For privacy reasons, before starting your analysis, you need to remove the 'SSN' column.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "# Sample dataset of patient records\n",
    "patient_data = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Gender': ['Female', 'Male', 'Male'],\n",
    "    'SSN': ['123-45-6789', '234-56-7890', '345-67-8901']\n",
    "})\n",
    "\n",
    "# Dropping the 'SSN' column for privacy\n",
    "sanitized_data = patient_data.drop(columns=['SSN'])\n",
    "print(sanitized_data)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "      Name  Age  Gender\n",
    "0    Alice   25  Female\n",
    "1      Bob   30    Male\n",
    "2  Charlie   35    Male\n",
    "```\n",
    "\n",
    "By using the `drop()` method, you've effectively removed sensitive information, ensuring that your subsequent data operations respect patient privacy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `set_index()`\n",
    "\n",
    "**Use**: The `set_index()` method allows you to set a DataFrame column as the index (i.e., row labels) of the DataFrame. It's particularly useful when you have a column that represents a unique identifier for rows and you want to use this column for faster data access or other operations.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "df_with_new_index = df.set_index('column_name')\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a librarian overseeing a database of books. Each book has an associated unique ISBN (International Standard Book Number). For easier searching and organization, you decide to set the 'ISBN' column as the index for your dataset.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "# Sample dataset of books with their titles and ISBN\n",
    "books_data = pd.DataFrame({\n",
    "    'Title': ['The Great Gatsby', 'Moby Dick', '1984'],\n",
    "    'Author': ['F. Scott Fitzgerald', 'Herman Melville', 'George Orwell'],\n",
    "    'ISBN': ['9780743273565', '9781503280786', '9780451524935']\n",
    "})\n",
    "\n",
    "# Setting the 'ISBN' column as the index\n",
    "books_with_index = books_data.set_index('ISBN')\n",
    "print(books_with_index)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "               Title                Author\n",
    "ISBN                                     \n",
    "9780743273565  The Great Gatsby  F. Scott Fitzgerald\n",
    "9781503280786  Moby Dick        Herman Melville\n",
    "9780451524935  1984             George Orwell\n",
    "```\n",
    "\n",
    "By employing the `set_index()` method, you've restructured the dataset so that each row (book) is easily accessible by its unique ISBN, streamlining operations like searches and updates.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `value_counts()`\n",
    "\n",
    "**Use**: The `value_counts()` method is applied to a pandas Series to compute a histogram of its values. It returns a Series of the counts of unique values sorted in descending order. This method is especially useful for getting a quick overview of the distribution of categorical data.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "value_distribution = df['column_name'].value_counts()\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're a marketing manager who recently conducted a survey to understand the preferred color of a new product among potential customers. The survey allowed participants to select one color from options like 'Red', 'Blue', 'Green', and 'Yellow'. You want to see a summary of how many respondents chose each color.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "# Sample dataset of survey responses\n",
    "survey_responses = pd.Series(['Red', 'Blue', 'Green', 'Red', 'Red', 'Blue', 'Yellow', 'Green', 'Green'])\n",
    "\n",
    "# Counting the occurrences of each color preference\n",
    "color_preference = survey_responses.value_counts()\n",
    "print(color_preference)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "Red       3\n",
    "Green     3\n",
    "Blue      2\n",
    "Yellow    1\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "By utilizing the `value_counts()` method, you can quickly discern that 'Red' and 'Green' are the most preferred colors, each selected by three respondents, providing valuable feedback for product design.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `unique()`\n",
    "\n",
    "**Use**: The `unique()` method is applied to a pandas Series to extract the distinct values. It's useful when you want to identify the unique values in a column without concern for their frequencies, which is different from `value_counts()` that gives you the frequencies of each unique value.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "distinct_values = df['column_name'].unique()\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You're an event manager, and you've been collecting RSVPs for an upcoming conference. Attendees were asked to specify which country they're coming from. As the event approaches, you need to prepare welcome kits tailored to attendees from different countries. To do this, you want to identify all the unique countries from which attendees are arriving.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "# Sample dataset of RSVPs with attendee country information\n",
    "rsvps = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "    'Country': ['USA', 'Canada', 'USA', 'UK', 'Canada']\n",
    "})\n",
    "\n",
    "# Extracting unique countries from the RSVPs\n",
    "unique_countries = rsvps['Country'].unique()\n",
    "print(unique_countries)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "['USA', 'Canada', 'UK']\n",
    "```\n",
    "\n",
    "By using the `unique()` method, you've quickly identified the distinct countries of the attendees, allowing you to prepare appropriate welcome kits for each group.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pivot_table()`\n",
    "\n",
    "**Use**: The `pivot_table()` method is used to reshape data by creating a spreadsheet-style pivot table as a DataFrame. It provides a way to aggregate, transform, and reorganize data in a tabular format, based on specified columns.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "pivot_df = df.pivot_table(index='row_column', columns='column_column', values='data_column', aggfunc='mean')\n",
    "```\n",
    "\n",
    "**Real-world Scenario**:  \n",
    "You manage sales for a chain of stores and have a dataset capturing daily sales for various products across different stores. You want to understand the average sales of each product per store. Instead of a long list, you prefer a summarized table where rows represent stores, columns represent products, and values show the average sales.\n",
    "\n",
    "**Example Code**:  \n",
    "```python\n",
    "# Sample dataset of daily sales for products in different stores\n",
    "sales_data = pd.DataFrame({\n",
    "    'Store': ['A', 'A', 'B', 'B', 'A', 'B'],\n",
    "    'Product': ['Widget', 'Gadget', 'Widget', 'Gadget', 'Widget', 'Gadget'],\n",
    "    'Sales': [100, 150, 90, 110, 105, 120]\n",
    "})\n",
    "\n",
    "# Creating a pivot table to get average sales per product per store\n",
    "pivot_sales = sales_data.pivot_table(index='Store', columns='Product', values='Sales', aggfunc='mean')\n",
    "print(pivot_sales)\n",
    "```\n",
    "\n",
    "**Output**:  \n",
    "```\n",
    "Product  Gadget  Widget\n",
    "Store                  \n",
    "A          150    102.5\n",
    "B          115     90.0\n",
    "```\n",
    "\n",
    "By employing the `pivot_table()` method, you've transformed the raw sales data into a structured and easily interpretable format, providing a clear view of product performance across stores.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Best Practices`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Use Vectorized Operations**:\n",
    "Pandas is built on top of NumPy, so it benefits from the efficient array operations that NumPy provides. Always prefer vectorized operations over applying functions using loops.\n",
    "    - Instead of: \n",
    "        ```python\n",
    "        for i in range(len(df)):\n",
    "            df['A'][i] += 5\n",
    "        ```\n",
    "    - Use:\n",
    "        ```python\n",
    "        df['A'] += 5\n",
    "        ```\n",
    "\n",
    "2. **Limit the Use of `apply()`**: The apply() method is flexible and powerful, but it can be slow, especially when used on large DataFrames. Whenever possible, use pandas' built-in vectorized methods instead.\n",
    "    - Instead of:\n",
    "        ```python\n",
    "        df['A'] = df['A'].apply(lambda x: x*2)\n",
    "        ```\n",
    "    - Use:\n",
    "        ```python\n",
    "        df['A'] *= 2\n",
    "        ```\n",
    "\n",
    "3. **Filter Early**:  If you’re working with a large dataset and you know you'll need only a subset of it, filter the data as early as possible in your operations. This reduces the amount of data you're working with.\n",
    "    - Instead of:\n",
    "        ```python\n",
    "        df = df[df['A'] > 5]\n",
    "        result = df.sum()\n",
    "        ```\n",
    "    - Use:\n",
    "        ```python\n",
    "        result = df[df['A'] > 5].sum()\n",
    "        ```\n",
    "\n",
    "4. **Use Appropriate Data Types**: Memory usage can be optimized by using appropriate data types. For instance, if you have a column of integers with values between 0 and 100, you can use the uint8 data type instead of the default int64, saving memory.\n",
    "    - Convert to appropriate type:\n",
    "        ```python\n",
    "        df['B'] = df['B'].astype('uint8')\n",
    "        ```\n",
    "\n",
    "5. **Avoid Chained Indexing**: Instead of df[df['A'] > 2]['B'] = new_val, use df.loc[df['A'] > 2, 'B'] = new_val. The former can produce unpredictable results or introduce performance costs.\n",
    "    - Instead of:\n",
    "        ```python\n",
    "        df[df['A'] > 2]['B'] = 5\n",
    "        ```\n",
    "    - Use:\n",
    "        ```python\n",
    "        df.loc[df['A'] > 2, 'B'] = 5\n",
    "        ```\n",
    "\n",
    "6. **Use `Category` Data Type for Categorical Data**: If a column in your DataFrame has a limited set of possible values (like 'Male', 'Female' for gender), converting it to a categorical type can save memory.\n",
    "    - Convert to category:\n",
    "        ```python\n",
    "        df['gender'] = df['gender'].astype('category')\n",
    "        ```\n",
    "\n",
    "7. **Evaluate Expression with `eval()`**: For some operations, using pd.eval() can be faster than traditional methods because it avoids intermediate array allocations.\n",
    "    - Using eval for faster operations:\n",
    "        ```python\n",
    "        result = pd.eval('df.A + df.B')\n",
    "        ```\n",
    "\n",
    "8. **Use `inplace` Parameter Carefully**: Some pandas methods have an inplace parameter. While using inplace=True might seem like a way to save memory, it doesn't always do so. Sometimes, it's faster and more memory-efficient to use the default inplace=False and assign the result to the original variable.\n",
    "    - Instead of:\n",
    "        ```python\n",
    "        df.drop(columns=['A'], inplace=True)\n",
    "        ```\n",
    "    - Use:\n",
    "        ```python\n",
    "        df = df.drop(columns=['A'])\n",
    "        ```\n",
    "\n",
    "9. **Take Advantage of Indexing**: Setting appropriate indexes, especially on large datasets, can drastically speed up operations like lookups, merges, and groupbys\n",
    "    - Set index for faster lookups:\n",
    "        ```python\n",
    "        df.set_index('name', inplace=True)\n",
    "        ```\n",
    "\n",
    "10. **Be Cautious with MultiIndexes**: While MultiIndexes can be powerful for certain tasks, they can also introduce overhead and complexity. Use them when necessary, but understand their trade-offs.\n",
    "    - Using MultiIndex:\n",
    "        ```python\n",
    "        arrays = [list('ABCD'), list('EFGH')]\n",
    "        index = pd.MultiIndex.from_arrays(arrays)\n",
    "        df = pd.DataFrame({'A': range(4)}, index=index)\n",
    "        ```\n",
    "\n",
    "11. **Profile Your Code**: Python provides built-in tools like cProfile for profiling your code. This can help you identify bottlenecks and inefficient sections of your code that you can then optimize.\n",
    "    - Use cProfile:\n",
    "        ```python\n",
    "        import cProfile\n",
    "        cProfile.run(\"df['A'] += 5\")\n",
    "        ```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
